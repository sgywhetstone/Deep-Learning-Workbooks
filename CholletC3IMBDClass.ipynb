{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CholletC3IMBDClass.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgywhetstone/CholletDLWorkbooks/blob/master/CholletC3IMBDClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IDpJ3ASN9zC",
        "colab_type": "text"
      },
      "source": [
        "This is a simple binary classification task taken from C3.4 of Chollet. This network classifies IMDB movie reviews into 'positive' and 'negative' categories. Training is done on 25k reviews taken from the Keras IMBD dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-2LgCB3QiUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f78cfeb-ac93-4ac8-fce3-792966b59b3c"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "keras.__version__"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZLvXCqFThrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set the number of words we keep from the reviews - e.g. if max_words = 10000, we keep only the top 10 000 most common words\n",
        "max_words = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvfwlHpgRtvq",
        "colab_type": "text"
      },
      "source": [
        "7/3/19 - Change in numpy leads to book implementation failing to load data. Fix from https://stackoverflow.com/a/56243777 \n",
        "\n",
        "Orginal implementation retains only lines 1 and 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWEPFCBbOXMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# save np.load\n",
        "np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words= max_words)\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "np.load = np_load_old"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLhAEjXAO_l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This decodes a selected review\n",
        "#word_index is a dictionary (word, index)\n",
        "word_index = imdb.get_word_index()\n",
        "#flip the dictionary\n",
        "reverse_word_index = dict(\n",
        "  [(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join(\n",
        "[reverse_word_index.get(i-3, '?') for i in train_data[24000]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27KCrqYtPPXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode the reviews via one-hot encoding\n",
        "def vectorize_sequences(sequences, dimension = max_words):  #One-hot function\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "#Vectorize training and test data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "#Vectorize training and test labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni01LDQ9VAGn",
        "colab_type": "text"
      },
      "source": [
        "We build a 3 layer network via Sequential - Two 16-dimensional relu layers, and one sigmoid layer to output probability. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ3J7DLzSNDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezLmLvxaUiVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5ca2ad44-f653-4d4e-b5b9-d370341fd9cf"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0704 00:18:24.492415 140302959847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0704 00:18:24.522816 140302959847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0704 00:18:24.532229 140302959847296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc8hDOYxWLMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}